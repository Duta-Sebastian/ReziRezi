from langchain_core.documents import Document
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_google_vertexai import ChatVertexAI
from utils import visualize_graph_documents
from typing import List 
from langchain_community.graphs.graph_document import GraphDocument, Node, Relationship
from pathlib import Path 
import asyncio 
import json 


llm = ChatVertexAI(model_name = 'gemini-2.0-flash-lite', project = "gen-lang-client-0628706774")

# These could be generated by AI:
allowed_nodes = ["Person", "Organization", "Location"]
allowed_relationships = [
    ("Person", "FRIEND", "Person"),
    ("Person", "FAMILY", "Person"),
    ("Person", "WORKS_WITH", "Person"),
    ("Person", "IS_PART_OF", "Organization")
]
node_properties=["political_party", "was_in_romanian_communist_party", "was_in_romanian_secret_services"]

def enhance_graph_prompt(initial_documents: List[Document], all_nodes: List[Node], all_relationships: List[Relationship])-> str:
    return (
            f"Original Text:\n{initial_documents[0].page_content}\n\n"
            f"We have already extracted the following graph. Review it and extract "
            f"any additional nodes, relationships and properties from the original text that might be missing.\n\n"
            f"Current Nodes:\n{all_nodes}\n\n"
            f"Current Relationships:\n{all_relationships}"
        )

async def iterative_graph_development(number_iterations: int, initial_documents: List[Document],
                                      initial_nodes: List[Node], initial_relationships: List[Relationship]) -> List[GraphDocument]:
    # Use a single, consistent transformer instance.
    # It uses its default prompt which is designed to find entities and relationships.
    transformer = LLMGraphTransformer(llm=llm, node_properties= True, relationship_properties=True)

    # Start with the initial documents
    current_documents = initial_documents
    graph = None
    all_nodes: List[Node] = initial_nodes
    all_relationships: List[Relationship] = initial_relationships
    initial_page_content = Document(page_content = initial_documents[0].page_content)
    for i in range(number_iterations):
        print(f"--- Iteration {i+1} ---")
        # The core of the transformer: convert documents to a graph
        graph_documents = await transformer.aconvert_to_graph_documents(current_documents)
        graph = graph_documents[0]
        
        print(f"Nodes: {graph.nodes}")
        print(f"Relationships: {graph.relationships}")
        all_nodes.extend(graph.nodes)
        all_relationships.extend(graph.relationships)
        # For the next iteration, create a new "richer" document.
        # This new document tells the LLM what it found so far and asks it to refine
        # or add to it based on the original content.
        
        # Combine the original text with the current graph state
        updated_content = enhance_graph_prompt(initial_documents, all_nodes, all_relationships)
        
        # Create a new Document object for the next loop
        current_documents = [Document(page_content=updated_content)]

    # Return the final graph from the last iteration
    return [GraphDocument(nodes=all_nodes, relationships=all_relationships,source = initial_page_content)]

def get_file_number(folder_path:str) -> int:
    return len([_ for _ in Path(folder_path).iterdir()])

async def main():
    graph_number = int(input("What graph do you want to start from?"))
    iteration_number = int(input("How many iterations?"))
    curr_number = get_file_number("curr_graphs") + 1
    documents: List[Document] = []
    graph_doc = None 
    if graph_number == -1:
        try:
            files = ["/home/stefan-taga/Desktop/Hackathon/ReziRezi/transcripts/A inventat Șoșoliberalismul： Rareș Bogdan, imaginea demagogiei în România ｜ Starea Impostorilor #50_transcript.txt"]
            #files = Path("transcripts").iterdir()
            for file in files:
                with open(file, mode = 'r') as file:
                    documents.append(Document(page_content = file.read()))
            
            if not documents:
                print("Error: 'transcripts' directory is empty or missing files.")
                return

        except FileNotFoundError:
            print("Error: The 'transcripts' directory was not found. Please create it and add text files.")
            return

        documents = [Document(page_content ="\n".join([document.page_content for document in documents]))]
        
        assert (len(documents) == 1)
    else:
        
        with open(f"curr_graphs/rel_graph_{graph_number}.json", mode='r') as file:
            graph_doc = GraphDocument.model_validate(json.load(file))
            documents.append(Document(page_content=enhance_graph_prompt([graph_doc.source],graph_doc.nodes,graph_doc.relationships)))

    print("Extracting graph data from document...")
    
    data = await iterative_graph_development(iteration_number,documents, initial_nodes= [] if graph_doc is None else graph_doc.nodes,
                                             initial_relationships= [] if graph_doc is None else graph_doc.relationships)

    with open(f"curr_graphs/rel_graph_{curr_number}.json", mode = 'w') as file:
        json.dump(data[0].model_dump(), file)

    if data:
        visualize_graph_documents(data) 
    else:
        print("No graph data extracted.")

if __name__ == "__main__":
    # Ensure you've run 'pip install networkx pyvis'
    asyncio.run(main())